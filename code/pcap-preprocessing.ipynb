{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["pip install scapy pandas"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mybemTJ4CDvR","executionInfo":{"status":"ok","timestamp":1764024161280,"user_tz":300,"elapsed":12708,"user":{"displayName":"Samia Choueiri","userId":"05651320646148324477"}},"outputId":"d2acdc33-cd0f-4f1f-e081-a9e4716407da"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting scapy\n","  Downloading scapy-2.6.1-py3-none-any.whl.metadata (5.6 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n","Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n","Downloading scapy-2.6.1-py3-none-any.whl (2.4 MB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/2.4 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: scapy\n","Successfully installed scapy-2.6.1\n"]}]},{"cell_type":"markdown","source":["http://cicresearch.ca/CICDataset/CIC-IDS-2017/Dataset/CIC-IDS-2017/PCAPs/ </br>\n","http://cicresearch.ca/CICDataset/CIC-IDS-2017/Dataset/CIC-IDS-2017/PCAPs/Friday-WorkingHours.pcap </br>\n","http://cicresearch.ca/CICDataset/CIC-IDS-2017/Dataset/CIC-IDS-2017/CSVs/GeneratedLabelledFlows.zip : In TrafficLabelling Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv"],"metadata":{"id":"Mchspr9mDQnD"}},{"cell_type":"code","source":["wget http://cicresearch.ca/IOTDataset/CIC_IOT_Dataset2023/Dataset/PCAP/Benign_Final/BenignTraffic.pcap.pcap\n","wget http://cicresearch.ca/IOTDataset/CIC_IOT_Dataset2023/Dataset/PCAP/DDoS-HTTP_Flood/DDoS-HTTP_Flood-.pcap"],"metadata":{"id":"DPQXhwbpJoMs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["BenignTraffic.pcap\n","\n","DDoS-HTTP_Flood-.pcap"],"metadata":{"id":"OT2_BuG3DfgT"}},{"cell_type":"markdown","source":["This script splits the output pcap files into 70% training and 30% testing pcap files\n","\n","\n","\n","```\n","python3 pcap_splitter_dpkt.py BenignTraffic.pcap ./pcaps_splits_10k --max-flows 50000 --train-flows 35000 --seed 42 --include-remaining none --verbose\n","python3 pcap_splitter_dpkt.py DDoS-HTTP_Flood.pcap ./pcaps_splits_10k --max-flows 50000 --train-flows 35000 --seed 42 --include-remaining none --verbose\n","```\n","\n"],"metadata":{"id":"eD0BEVoUGtFD"}},{"cell_type":"code","source":["Try AI directly in your favorite apps … Use Gemini to generate drafts and refine content, plus get Gemini Pro with access to Google's next-gen AI for $19.99 $0 for 1 month\n","#!/usr/bin/env python3\n","\"\"\"\n","Fast two-pass PCAP splitter using dpkt (optimized).\n","\n","Behavior:\n"," - By default: collect first `--max-flows` unique flows (overall),\n","   then second-pass scans entire pcap to write ALL packets belonging to those flows\n","   (ensures complete flows).\n"," - If --prefix-only is set: collect flows and only write packets seen up to the point\n","   where the N-th unique flow was discovered (NO full second pass). This is much faster,\n","   but you won't get packets for those flows that appear later in the file.\n","\n","Usage:\n","  python3 pcap_splitter_dpkt.py DDoS-SYN_Flood.pcap ./pcaps_splits \\\n","      --max-flows 10000 --train-flows 7000 --seed 42 --include-remaining none --verbose\n","\n","  # Fast prefix mode (stop at the packet where the 10k-th flow was first seen):\n","  python3 pcap_splitter_dpkt.py DDoS-SYN_Flood.pcap ./pcaps_splits \\\n","      --max-flows 10000 --train-flows 7000 --prefix-only --verbose\n","\"\"\"\n","import os, argparse, hashlib, struct\n","import dpkt, socket\n","\n","def flow_key_from_eth(eth, pkt_index):\n","    \"\"\"Return flow key as bytes; non-IP gets a unique sentinel using packet index.\"\"\"\n","    try:\n","        ip = eth.data\n","    except Exception:\n","        return b'__nonip__' + struct.pack(\"!I\", pkt_index)\n","    if not isinstance(ip, dpkt.ip.IP):\n","        return b'__nonip__' + struct.pack(\"!I\", pkt_index)\n","    src = ip.src  # bytes\n","    dst = ip.dst\n","    proto = ip.p\n","    sport = 0\n","    dport = 0\n","    if proto == dpkt.ip.IP_PROTO_TCP:\n","        try:\n","            tcp = ip.data\n","            sport = getattr(tcp, 'sport', 0)\n","            dport = getattr(tcp, 'dport', 0)\n","        except Exception:\n","            sport = 0; dport = 0\n","    elif proto == dpkt.ip.IP_PROTO_UDP:\n","        try:\n","            udp = ip.data\n","            sport = getattr(udp, 'sport', 0)\n","            dport = getattr(udp, 'dport', 0)\n","        except Exception:\n","            sport = 0; dport = 0\n","    # pack src(4) dst(4) sport(2) dport(2) proto(1)\n","    return src + dst + struct.pack(\"!HHB\", sport, dport, proto)\n","\n","def collect_flows_dpkt(path, max_flows=None, verbose=False, prefix_only=False):\n","    \"\"\"\n","    Collect unique flows in file order.\n","    If prefix_only=True, also store packets seen up to break point and return them.\n","    Returns:\n","      flow_order (list of flow keys),\n","      packets_seen (list of (ts, buf, key)) if prefix_only else None\n","    \"\"\"\n","    seen = set()\n","    flow_order = []\n","    packets_seen = [] if prefix_only else None\n","    total_pkts = 0\n","    with open(path, 'rb') as f:\n","        pcap = dpkt.pcap.Reader(f)\n","        for ts, buf in pcap:\n","            total_pkts += 1\n","            try:\n","                eth = dpkt.ethernet.Ethernet(buf)\n","                key = flow_key_from_eth(eth, total_pkts)\n","            except Exception:\n","                key = b'__nonip__' + struct.pack(\"!I\", total_pkts)\n","\n","            # If prefix-only, store packet until we break\n","            if prefix_only:\n","                packets_seen.append((ts, buf, key))\n","\n","            if key not in seen:\n","                seen.add(key)\n","                flow_order.append(key)\n","                if (max_flows is not None) and (len(flow_order) >= max_flows):\n","                    if verbose:\n","                        print(f\"  reached max_flows={max_flows} at packet #{total_pkts}\")\n","                    break\n","\n","            if verbose and (total_pkts % 500000 == 0):\n","                print(f\"  scanned {total_pkts} pkts, collected {len(flow_order)} flows\")\n","    if verbose:\n","        print(f\"collect_flows: scanned {total_pkts} pkts, collected {len(flow_order)} flows\")\n","    return flow_order, packets_seen\n","\n","def flow_score(flow_key, seed):\n","    m = hashlib.md5()\n","    m.update(str(seed).encode('utf-8'))\n","    m.update(flow_key)\n","    return int.from_bytes(m.digest(), 'big')\n","\n","def choose_train(flow_keys, k, seed):\n","    if k >= len(flow_keys):\n","        return set(flow_keys)\n","    scored = [(flow_score(kf, seed), kf) for kf in flow_keys]\n","    scored.sort(key=lambda x: x[0])\n","    sel = {kf for (_, kf) in scored[:k]}\n","    return sel\n","\n","def write_from_prefix(packets_seen, train_set, collected_set, out_folder, base, include_remaining=\"none\", verbose=False):\n","    \"\"\"Write train/test pcaps using only packets in packets_seen list (faster).\"\"\"\n","    import dpkt, os\n","    train_out = os.path.join(out_folder, base + \"_train.pcap\")\n","    test_out = os.path.join(out_folder, base + \"_test.pcap\")\n","    os.makedirs(out_folder, exist_ok=True)\n","    tf = open(train_out, 'wb'); tw = dpkt.pcap.Writer(tf)\n","    ff = open(test_out, 'wb'); tw2 = dpkt.pcap.Writer(ff)\n","\n","    ntrain = 0; ntest = 0; total = 0\n","    for ts, buf, key in packets_seen:\n","        total += 1\n","        if key in train_set:\n","            tw.writepkt(buf, ts=ts); ntrain += 1\n","        else:\n","            if key in collected_set:\n","                tw2.writepkt(buf, ts=ts); ntest += 1\n","            else:\n","                if include_remaining == \"train\":\n","                    tw.writepkt(buf, ts=ts); ntrain += 1\n","                elif include_remaining == \"test\":\n","                    tw2.writepkt(buf, ts=ts); ntest += 1\n","                else:\n","                    pass\n","    tf.close(); ff.close()\n","    if verbose:\n","        print(f\"Prefix write -> total pkts considered: {total}, train:{ntrain}, test:{ntest}\")\n","    return train_out, test_out, ntrain, ntest\n","\n","def write_split_dpkt_fullscan(path, out_folder, train_set, collected_keys_set, include_remaining=\"none\", verbose=False):\n","    \"\"\"Second-pass full-scan writer (writes packets belonging to selected flows anywhere in file).\"\"\"\n","    base = os.path.splitext(os.path.basename(path))[0]\n","    os.makedirs(out_folder, exist_ok=True)\n","    train_out = os.path.join(out_folder, base + \"_train.pcap\")\n","    test_out = os.path.join(out_folder, base + \"_test.pcap\")\n","    tw_f = open(train_out, 'wb'); tw = dpkt.pcap.Writer(tw_f)\n","    tt_f = open(test_out, 'wb'); tt = dpkt.pcap.Writer(tt_f)\n","\n","    total = 0; ntrain = 0; ntest = 0\n","    with open(path, 'rb') as f:\n","        pcap = dpkt.pcap.Reader(f)\n","        for ts, buf in pcap:\n","            total += 1\n","            try:\n","                eth = dpkt.ethernet.Ethernet(buf)\n","                key = flow_key_from_eth(eth, total)\n","            except Exception:\n","                key = b'__nonip__' + struct.pack(\"!I\", total)\n","\n","            if key in train_set:\n","                tw.writepkt(buf, ts=ts); ntrain += 1\n","            else:\n","                if key in collected_keys_set:\n","                    tt.writepkt(buf, ts=ts); ntest += 1\n","                else:\n","                    if include_remaining == \"train\":\n","                        tw.writepkt(buf, ts=ts); ntrain += 1\n","                    elif include_remaining == \"test\":\n","                        tt.writepkt(buf, ts=ts); ntest += 1\n","                    else:\n","                        pass\n","\n","            if verbose and (total % 500000 == 0):\n","                print(f\"  scanned {total} pkts, ntrain={ntrain}, ntest={ntest}\")\n","    tw_f.close(); tt_f.close()\n","    if verbose:\n","        print(f\"Full-scan write -> scanned {total}, train:{ntrain}, test:{ntest}\")\n","    return train_out, test_out, ntrain, ntest\n","\n","def main():\n","    ap = argparse.ArgumentParser(description=\"Fast dpkt-based pcap splitter (prefix-only option)\")\n","    ap.add_argument(\"pcap\", help=\"input pcap file\")\n","    ap.add_argument(\"out_folder\", help=\"output folder\")\n","    ap.add_argument(\"--max-flows\", type=int, default=10000, help=\"total flows to collect (use 0 for all)\")\n","    ap.add_argument(\"--train-flows\", type=int, default=7000, help=\"train flows within collected\")\n","    ap.add_argument(\"--seed\", type=int, default=42)\n","    ap.add_argument(\"--include-remaining\", choices=[\"none\",\"train\",\"test\"], default=\"none\")\n","    ap.add_argument(\"--prefix-only\", action=\"store_true\", help=\"Only use packets up to the point where max-flows was reached (fast).\")\n","    ap.add_argument(\"--verbose\", action=\"store_true\")\n","    args = ap.parse_args()\n","\n","    max_flows = args.max_flows if args.max_flows > 0 else None\n","\n","    if args.verbose:\n","        print(\"Collecting flows (this stops when first max_flows unique flows are found)...\")\n","    flow_keys, packets_seen = collect_flows_dpkt(args.pcap, max_flows=max_flows, verbose=args.verbose, prefix_only=args.prefix_only)\n","\n","    if len(flow_keys) == 0:\n","        print(\"No flows collected; exiting.\")\n","        return\n","\n","    if args.train_flows > len(flow_keys):\n","        print(f\"Warning: requested train {args.train_flows} > collected {len(flow_keys)}. Selecting all available flows for train.\")\n","    train_set = choose_train(flow_keys, args.train_flows, args.seed)\n","    collected_set = set(flow_keys)\n","\n","    base = os.path.splitext(os.path.basename(args.pcap))[0]\n","\n","    if args.prefix_only:\n","        if args.verbose:\n","            print(\"Writing split using only the prefix (packets captured up to the break point)...\")\n","        train_out, test_out, ntrain, ntest = write_from_prefix(packets_seen, train_set, collected_set,\n","                                                               args.out_folder, base, include_remaining=args.include_remaining, verbose=args.verbose)\n","    else:\n","        if args.verbose:\n","            print(\"Writing split by full-scan (this will scan the entire pcap to include complete flows)...\")\n","        train_out, test_out, ntrain, ntest = write_split_dpkt_fullscan(args.pcap, args.out_folder, train_set, collected_set, include_remaining=args.include_remaining, verbose=args.verbose)\n","\n","    if args.verbose:\n","        print(f\"Done. Outputs:\\n  {train_out} ({ntrain} pkts)\\n  {test_out} ({ntest} pkts)\")\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"id":"VqfjvT6YEBDe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["DDoS-HTTP_Flood-_test.pcap\n","\n","BenignTraffic_test.pcap\n","\n","DDoS-HTTP_Flood-_train.pcap\n","\n","BenignTraffic_train.pcap"],"metadata":{"id":"fgPch-a5HTjl"}},{"cell_type":"markdown","source":["This bash script takes the output pcap and generates txt files:\n","\n","\n","```\n","chmod +x extract_pkts.sh\n","./extract_pkts.sh\n","```\n","\n"],"metadata":{"id":"b2NP166xHen_"}},{"cell_type":"code","source":["# Change the output file path based on test or train dataset\n","for f in *.pcap\n","\tdo\n","\t\techo $f\n","        tshark -r $f -Y 'ip.proto == 6 or ip.proto == 17' -T fields -e frame.time_relative -e ip.src -e ip.dst -e tcp.srcport -e tcp.dstport -e ip.proto -e ip.len -e udp.srcport -e udp.dstport -E separator='|' > $f.txt\n","\tdone"],"metadata":{"id":"iqQ4_rlOEBS1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["DDoS-HTTP_Flood-_test.pcap.txt\n","\n","BenignTraffic_test.pcap.txt\n","\n","DDoS-HTTP_Flood-_train.pcap.txt\n","\n","BenignTraffic_train.pcap.txt"],"metadata":{"id":"gsxvzewqHtv9"}},{"cell_type":"markdown","source":["The python scrpt below is called by the shell script"],"metadata":{"id":"WIvs6KxOIqaX"}},{"cell_type":"code","source":["Try AI directly in your favorite apps … Use Gemini to generate drafts and refine content, plus get Gemini Pro with access to Google's next-gen AI for $19.99 $0 for 1 month\n","import pandas as pd\n","import numpy as np\n","import sys\n","import os\n","\n","# Read input arguments\n","filename_in = sys.argv[1]\n","filename_out = sys.argv[2]\n","npkts = int(sys.argv[3])\n","\n","# Load packet data\n","packet_data = pd.read_csv(\n","    filename_in,\n","    sep='|',\n","    header=None,\n","    dtype=str,\n","    low_memory=False\n",")\n","\n","# Assign columns (as in your original)\n","packet_data.columns = [\n","    'timestamp', 'ip.src', 'ip.dst',\n","    'tcp.srcport', 'tcp.dstport',\n","    'ip.proto', 'ip.len',\n","    'udp.srcport', 'udp.dstport'\n","]\n","\n","# Normalize column names to safe Python identifiers (no dots)\n","packet_data = packet_data.rename(columns={\n","    'ip.src': 'ip_src',\n","    'ip.dst': 'ip_dst',\n","    'tcp.srcport': 'tcp_srcport',\n","    'tcp.dstport': 'tcp_dstport',\n","    'ip.proto': 'ip_proto',\n","    'ip.len': 'ip_len',\n","    'udp.srcport': 'udp_srcport',\n","    'udp.dstport': 'udp_dstport'\n","})\n","\n","# Drop rows with missing proto and strip whitespace\n","packet_data['ip_proto'] = packet_data['ip_proto'].astype(str).str.strip()\n","packet_data = packet_data.dropna(subset=['ip_proto'])\n","\n","# Convert ip_len to int where possible\n","packet_data['ip_len'] = pd.to_numeric(packet_data['ip_len'], errors='coerce').fillna(0).astype(int)\n","\n","# Convert TCP and UDP ports to nullable integers (Int64)\n","packet_data['tcp_srcport'] = pd.to_numeric(packet_data['tcp_srcport'], errors='coerce').astype('Int64')\n","packet_data['tcp_dstport'] = pd.to_numeric(packet_data['tcp_dstport'], errors='coerce').astype('Int64')\n","packet_data['udp_srcport'] = pd.to_numeric(packet_data['udp_srcport'], errors='coerce').astype('Int64')\n","packet_data['udp_dstport'] = pd.to_numeric(packet_data['udp_dstport'], errors='coerce').astype('Int64')\n","\n","# Normalize protocol values to simple strings '6' and '17' (or others)\n","packet_data['ip_proto'] = packet_data['ip_proto'].str.strip()\n","\n","# Build srcport/dstport based on protocol (strings compared to '6'/'17')\n","packet_data['srcport'] = np.where(packet_data['ip_proto'] == '6',\n","                                  packet_data['tcp_srcport'],\n","                                  packet_data['udp_srcport'])\n","packet_data['dstport'] = np.where(packet_data['ip_proto'] == '6',\n","                                  packet_data['tcp_dstport'],\n","                                  packet_data['udp_dstport'])\n","\n","# Make sure srcport/dstport are Int64\n","packet_data['srcport'] = packet_data['srcport'].astype('Int64')\n","packet_data['dstport'] = packet_data['dstport'].astype('Int64')\n","\n","# Optional: filter out weird combined values like \"1,17\" only if you intended that originally.\n","# (Your original filter `packet_data[\"ip.proto\"] != \"1,17\"` etc. looked accidental.)\n","# If you want to remove rows where ip_proto contains a comma:\n","packet_data = packet_data[~packet_data['ip_proto'].str.contains(',', na=False)].reset_index(drop=True)\n","\n","# Create safe flow_id column (use underscores)\n","packet_data['flow_id'] = (\n","    packet_data['ip_src'].astype(str) + \" \" +\n","    packet_data['ip_dst'].astype(str) + \" \" +\n","    packet_data['srcport'].astype(str) + \" \" +\n","    packet_data['dstport'].astype(str) + \" \" +\n","    packet_data['ip_proto'].astype(str)\n",")\n","\n","# Labeling based on filename\n","filename_patterns = {\"ddos\": \"DDoS\", \"benign\": \"BENIGN\"}\n","label = \"UNKNOWN\"\n","for pattern, label_in_filename in filename_patterns.items():\n","    if pattern in filename_in.lower():\n","        label = label_in_filename\n","\n","number_of_pkts_limit = npkts\n","min_number_of_packets = npkts\n","\n","# Initialize dictionaries\n","main_packet_size = {}\n","flow_list = []\n","differential_packet_size = {}\n","main_inter_arrival_time = {}\n","last_time = {}\n","avg_pkt_sizes = {}\n","labels = {}\n","flow_start_time = {}\n","flow_end_time = {}\n","\n","# Collect packets into flows\n","print(\"NOW: COLLECTING PACKETS INTO FLOWS...\")\n","# Use named tuple attributes to avoid index mistakes\n","for row in packet_data.itertuples(index=False, name='Row'):\n","    # access via attributes (note: dot in original column becomes underscore in attribute)\n","    time = float(row.timestamp)\n","    srcip = row.ip_src\n","    dstip = row.ip_dst\n","    pktsize = int(row.ip_len)       # packet length from ip_len\n","    proto = str(row.ip_proto)       # '6' or '17' etc.\n","    srcport = row.srcport           # Int64 or <NA>\n","    dstport = row.dstport\n","    key = row.flow_id\n","\n","    if key in flow_list:\n","        if len(main_packet_size[key]) < number_of_pkts_limit:\n","            main_packet_size[key].append(pktsize)\n","            flow_end_time[key] = time\n","\n","            # differential packet length (previous exists because this is not first append)\n","            prev = main_packet_size[key][-2]\n","            diff_len = abs(pktsize - prev)\n","            differential_packet_size[key].append(diff_len)\n","            labels[key] = label\n","\n","            # IAT\n","            lasttime = last_time[key]\n","            diff = round(float(time) - float(lasttime), 9)\n","            main_inter_arrival_time[key].append(diff)\n","            last_time[key] = time\n","    else:\n","        # new flow\n","        flow_list.append(key)\n","        labels[key] = label\n","        main_packet_size[key] = [pktsize]\n","        main_inter_arrival_time[key] = []\n","        differential_packet_size[key] = []\n","        flow_start_time[key] = time\n","        flow_end_time[key] = time\n","        last_time[key] = time\n","\n","# Write output to CSV (include src/dst ports in header and output)\n","print(\"NOW: WRITING FLOW FEATURES INTO CSV...\")\n","header = \"Flow ID,Src Port,Dst Port,Min Packet Length,Max Packet Length,Packet Length Total,Min differential Packet Length,Max differential Packet Length,IAT min,IAT max,Flow Duration,Label\"\n","file_exists = os.path.isfile(filename_out)\n","with open(filename_out, \"a\") as text_file:\n","    if not file_exists:\n","        text_file.write(header + \"\\n\")\n","\n","    for key in flow_list:\n","        packet_list = main_packet_size[key]\n","        length_packets = len(packet_list)\n","        total_bytes = sum(packet_list)\n","        avg_pkt_sizes[key] = total_bytes / length_packets\n","        min_pkt_size = min(packet_list)\n","        max_pkt_size = max(packet_list)\n","\n","        inter_arrival_time_list = main_inter_arrival_time[key]\n","        iat_len = len(inter_arrival_time_list)\n","        if iat_len == 0:\n","            min_IAT_ms = 0\n","            max_IAT_ms = 0\n","            flow_duration_ms = 0\n","        else:\n","            min_IAT = min(inter_arrival_time_list)\n","            max_IAT = max(inter_arrival_time_list)\n","            min_IAT_ms = round(1000000 * min_IAT, 9)\n","            max_IAT_ms = round(1000000 * max_IAT, 9)\n","            flow_duration = sum(inter_arrival_time_list)\n","            flow_duration_ms = round(1000000000 * flow_duration, 9)\n","\n","        min_diff_pkt_size = min(differential_packet_size[key]) if differential_packet_size[key] else 0\n","        max_diff_pkt_size = max(differential_packet_size[key]) if differential_packet_size[key] else 0\n","\n","        # parse src/dst port from flow_id or from the packet_data (preferred)\n","        # extract ports from flow_id string: split on spaces -> [srcip,dstip,srcport,dstport,proto]\n","        try:\n","            parts = key.split(\" \")\n","            srcport_str = parts[2]\n","            dstport_str = parts[3]\n","        except Exception:\n","            srcport_str = \"\"\n","            dstport_str = \"\"\n","\n","        out_line = (\n","            f\"{key},{srcport_str},{dstport_str},\"\n","            f\"{min_pkt_size},{max_pkt_size},{total_bytes},\"\n","            f\"{min_diff_pkt_size},{max_diff_pkt_size},\"\n","            f\"{min_IAT_ms},{max_IAT_ms},{flow_duration_ms},{labels[key]}\\n\"\n","        )\n","\n","        if len(main_packet_size[key]) >= min_number_of_packets:\n","            text_file.write(out_line)\n"],"metadata":{"id":"K47IMb-nEo73"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This shell script take the txt files as input and generate CSV files with all computed features per flow\n","\n","```\n","chmod +x extract_flows.sh\n","./extract_flows.sh\n","```\n","\n","Note: this is wrong file refrencing. They need to be processed in separate directories (train and test)"],"metadata":{"id":"zU3Yq1guIzJ_"}},{"cell_type":"code","source":["#!/bin/bash\n","\n","# Change the output file path based on test or train dataset\n","output_file=\"train_data_CIC.csv\"\n","\n","if [ ! -f $output_file ]; then\n","    echo \"Flow ID, Min Packet Length, Max Packet Length, Packet Length Total, Min differential Packet Length, Max differential Packet Length, IAT min, IAT max, Flow Duration, Label\" > $output_file\n","fi\n","\n","for f in *.txt\n","    do\n","        echo \"Processing $f\"\n","        python3 extract_flows_from_txt.py $f $output_file 4\n","    done\n","\n","echo \"Feature extraction completed for all files.\""],"metadata":{"id":"jjx_FmHXEBcQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["train_data_CIC.csv </br>\n","test_data_CIC.csv </br>\n","Those files are found in the google drive"],"metadata":{"id":"u_hXvhqYJJdI"}}]}